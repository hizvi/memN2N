{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1368,
     "status": "ok",
     "timestamp": 1556824781039,
     "user": {
      "displayName": "Hasan Rizvi",
      "photoUrl": "",
      "userId": "12377090762076158584"
     },
     "user_tz": 300
    },
    "id": "_5VJVJOqFsN5",
    "outputId": "2d3be709-280d-494c-f321-7aa404c4f21a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Dropout, Lambda, Flatten\n",
    "from keras.layers import Dot, Activation, Softmax, Add, Multiply, Permute\n",
    "from keras.layers import dot, add, multiply\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from src.util import *\n",
    "\n",
    "from keras.backend import variable, transpose, reshape, gather\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "import tarfile\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7O1EdrUWGrfs"
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SIR4LwoHCCN"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_aKSz3dH4cI"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'qa1': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    'qa2': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "    'qa3': 'tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_{}.txt',\n",
    "    'qa4': 'tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_{}.txt',\n",
    "    'qa5': 'tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_{}.txt',\n",
    "    'qa6': 'tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_{}.txt',\n",
    "    'qa7': 'tasks_1-20_v1-2/en-10k/qa7_counting_{}.txt',\n",
    "    'qa8': 'tasks_1-20_v1-2/en-10k/qa8_lists-sets_{}.txt',\n",
    "    'qa9': 'tasks_1-20_v1-2/en-10k/qa9_simple-negation_{}.txt',\n",
    "    'qa10': 'tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_{}.txt',\n",
    "    'qa11': 'tasks_1-20_v1-2/en-10k/qa11_basic-coreference_{}.txt',\n",
    "    'qa12': 'tasks_1-20_v1-2/en-10k/qa12_conjunction_{}.txt',\n",
    "    'qa13': 'tasks_1-20_v1-2/en-10k/qa13_compound-coreference_{}.txt',\n",
    "    'qa14': 'tasks_1-20_v1-2/en-10k/qa14_time-reasoning_{}.txt',\n",
    "    'qa15': 'tasks_1-20_v1-2/en-10k/qa15_basic-deduction_{}.txt',\n",
    "    'qa16': 'tasks_1-20_v1-2/en-10k/qa16_basic-induction_{}.txt',\n",
    "    'qa17': 'tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_{}.txt',\n",
    "    'qa18': 'tasks_1-20_v1-2/en-10k/qa18_size-reasoning_{}.txt',\n",
    "    'qa19': 'tasks_1-20_v1-2/en-10k/qa19_path-finding_{}.txt',\n",
    "    'qa20': 'tasks_1-20_v1-2/en-10k/qa20_agents-motivations_{}.txt',\n",
    "}\n",
    "\n",
    "challenge_type = 'qa{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrNtmWFdijxZ"
   },
   "outputs": [],
   "source": [
    "class MemN2NBlock(Layer): \n",
    "    def __init__(self, output_dim):\n",
    "        super(MemN2NBlock, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # layer operations\n",
    "        self.input_memory = Dot(axes=(-1))\n",
    "        self.input_representation = Softmax()\n",
    "        self.permute_weights = Permute((2,1))\n",
    "        self.output_memory = Dot(axes=(2,1))\n",
    "        self.h_mapping = self.add_weight(\n",
    "                name='H',\n",
    "                shape=(self.output_dim[2],output_dim[2]),\n",
    "                initializer='glorot_normal',\n",
    "                trainable=True\n",
    "        )\n",
    "        self.new_u = Add()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        m = self.input_memory([inputs[0], inputs[1]])\n",
    "        p = self.input_representation(m)\n",
    "        # print('p.shape: ', p.shape)\n",
    "        # p = self.permute_weights(p) \n",
    "        p = reshape(p, [-1, p.shape[2], p.shape[1]])\n",
    "        # print('p.shape: ', p.shape)\n",
    "        c = self.output_memory([p, inputs[2]])\n",
    "        \n",
    "        mapped_u = K.dot(inputs[1], self.h_mapping)\n",
    "        \n",
    "        return self.new_u([c, mapped_u])\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(MemN2NBlock, self).build(input_shape)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.output_dim\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsoBdPPuiT1f"
   },
   "outputs": [],
   "source": [
    "def build_model(story_maxlen, query_maxlen, vocab_size):\n",
    "    print('MEM N2N model')\n",
    "\n",
    "    HOPS = 3\n",
    "\n",
    "    sentences = Input((story_maxlen,))\n",
    "    question = Input((query_maxlen,))\n",
    "\n",
    "    A = Embedding(vocab_size, 64)(sentences)\n",
    "    B = Embedding(vocab_size, 64)(question)\n",
    "    C = Embedding(vocab_size, 64)(sentences)\n",
    "\n",
    "    u = B\n",
    "    u_shape = tuple(map(lambda x: x.value, u.shape))\n",
    "\n",
    "    for i in range(HOPS):\n",
    "        u = MemN2NBlock(output_dim=u_shape)([A, u, C])\n",
    "\n",
    "    # u = Lambda(lambda x: K.sum(x, axis=1))(u) \n",
    "    u = Flatten()(u)\n",
    "\n",
    "    result = Dense(vocab_size, activation='softmax', use_bias=False)(u)\n",
    "\n",
    "    print(result.shape)\n",
    "\n",
    "\n",
    "    MemN2Nmodel = Model(inputs=[sentences, question], outputs=result)\n",
    "    MemN2Nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return MemN2Nmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33370,
     "status": "ok",
     "timestamp": 1556824818528,
     "user": {
      "displayName": "Hasan Rizvi",
      "photoUrl": "",
      "userId": "12377090762076158584"
     },
     "user_tz": 300
    },
    "id": "o3jec4mRriyp",
    "outputId": "02a423f4-e236-421f-9af5-a3340928ff17"
   },
   "outputs": [],
   "source": [
    "train_stories = []\n",
    "val_stories = []\n",
    "\n",
    "test_stories = []\n",
    "test_flattened = []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    challenge = challenges[challenge_type.format(i)]\n",
    "\n",
    "    print('Extracting stories for the challenge:', challenge_type.format(i))\n",
    "    print(challenge.format(''))\n",
    "\n",
    "    with tarfile.open(path) as tar:\n",
    "        train_stories_q = get_stories(tar.extractfile(challenge.format('train')))\n",
    "        test_stories_q = get_stories(tar.extractfile(challenge.format('test')))\n",
    "        \n",
    "    train_stories_q, val_stories_q = train_test_split(train_stories_q, test_size=0.025)    \n",
    "\n",
    "    train_stories.extend(train_stories_q)\n",
    "    val_stories.extend(val_stories_q)\n",
    "    test_stories.append(train_stories_q)\n",
    "    test_flattened.extend(train_stories_q)\n",
    "      \n",
    "# Building vocabulary\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_flattened + val_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_flattened + val_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_flattened + val_stories)))\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories, word_idx, story_maxlen, query_maxlen)\n",
    "inputs_val, queries_val, answers_val = vectorize_stories(val_stories, word_idx, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ezd-09KgsRe8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3621
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5975223,
     "status": "ok",
     "timestamp": 1556830852227,
     "user": {
      "displayName": "Hasan Rizvi",
      "photoUrl": "",
      "userId": "12377090762076158584"
     },
     "user_tz": 300
    },
    "id": "JusaVJM1FsN_",
    "outputId": "40d5f4e5-1f33-4d91-c743-08e2f5a97edc"
   },
   "outputs": [],
   "source": [
    "model = build_model(story_maxlen, query_maxlen, vocab_size)\n",
    "\n",
    "history = model.fit([inputs_train, queries_train], answers_train,\n",
    "      batch_size=32,\n",
    "      epochs=100,\n",
    "      validation_data=([inputs_val, queries_val], answers_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25730,
     "status": "ok",
     "timestamp": 1556833532166,
     "user": {
      "displayName": "Hasan Rizvi",
      "photoUrl": "",
      "userId": "12377090762076158584"
     },
     "user_tz": 300
    },
    "id": "70M4qDNEFsOI",
    "outputId": "a0322dc7-f48e-425e-a8a9-e04d0e32edc7"
   },
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "for task in test_stories:\n",
    "    inputs_test, queries_test, answers_test = vectorize_stories(task, word_idx, story_maxlen, query_maxlen)\n",
    "    test_results.append(model.evaluate([inputs_test, queries_test], answers_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1556834108601,
     "user": {
      "displayName": "Hasan Rizvi",
      "photoUrl": "",
      "userId": "12377090762076158584"
     },
     "user_tz": 300
    },
    "id": "xfAciXnXfxvl",
    "outputId": "6de3b0ab-3e77-48ff-cda3-c4a169f9e7f9"
   },
   "outputs": [],
   "source": [
    "plt.bar(range(1, 21), np.array(test_results)[:,1])\n",
    "plt.xticks(range(1, 21))\n",
    "plt.title('Test Accuracies')\n",
    "plt.savefig('joint_accuracy_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLGV827yiOGY"
   },
   "outputs": [],
   "source": [
    "files.download('joint_accuracy_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5ZffCfepSae"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(5, 4, sharex='col', sharey='row')\n",
    "fig.suptitle('Accuracies')\n",
    "count = 0\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(4):\n",
    "    \n",
    "        cur_ax = ax[i, j]\n",
    "\n",
    "        cur_ax.set_ylim([0, 1])\n",
    "\n",
    "        cur_ax.figsize = (10,10)\n",
    "        cur_ax.plot(histories[count].history['acc'][:40])\n",
    "        cur_ax.plot(histories[count].history['val_acc'][:40])\n",
    "        cur_ax.set_title('Task {}'.format(count + 1))\n",
    "        cur_ax.legend(['train', 'test'], loc='upper left')\n",
    "        count += 1\n",
    "\n",
    "fig.savefig('accuracies.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ixd4UQOEDZga"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(5, 4, sharex='col', sharey='none')\n",
    "fig.suptitle('Losses')\n",
    "count = 0\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(4):\n",
    "\n",
    "        cur_ax = ax[i, j]\n",
    "\n",
    "        cur_ax.figsize = (10,10)\n",
    "        cur_ax.plot(histories[count].history['loss'][:40])\n",
    "        cur_ax.plot(histories[count].history['val_loss'][:40])\n",
    "        cur_ax.set_title('Task {}'.format(count + 1))\n",
    "        cur_ax.legend(['train', 'test'], loc='upper left')\n",
    "        count += 1\n",
    "\n",
    "fig.savefig('losses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWmASd_CzzDS"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('joint_accuracy.png')\n",
    "files.download('joint_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtqqT7jtDoee"
   },
   "outputs": [],
   "source": [
    "histories[9].history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Afv4CDwgFsOb"
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(MemN2Nmodel).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1556832934959,
     "user": {
      "displayName": "Hasan Rizvi",
      "photoUrl": "",
      "userId": "12377090762076158584"
     },
     "user_tz": 300
    },
    "id": "ilmU3KBgFsOe",
    "outputId": "d904d51c-8eb8-447a-82e6-056d549d2ddb"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(model.history.history['acc'])\n",
    "plt.plot(model.history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.savefig('joint_accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1556832950524,
     "user": {
      "displayName": "Hasan Rizvi",
      "photoUrl": "",
      "userId": "12377090762076158584"
     },
     "user_tz": 300
    },
    "id": "EiVeXMDIFsOh",
    "outputId": "86f5f89f-f455-4a8f-c36a-1fb1bfcdd09a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(model.history.history['loss'][:200])\n",
    "plt.plot(model.history.history['val_loss'][:200])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('joint_loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bxh0UhPqFsOl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WuYXjgv2tRaU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bAbi_jointly_trained.ipynb",
   "provenance": [
    {
     "file_id": "1W017ec2GYX4-NtbBT44c2MV8FQrb_6S6",
     "timestamp": 1556802691730
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
